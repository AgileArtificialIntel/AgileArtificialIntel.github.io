<h1 id="perceptron">Perceptron</h1>
<p>This chapters plays two roles. The first one, is to describes how and why a perceptron plays a role so important in the meaning of deep learning. The second role of this chapter, is to provide a gentle introduce to the Pharo programming language.</p>
<h2 id="biological-connection">Biological Connection</h2>
<p>The primary visual cortex contains 140 millions of neurons, with tens of billions of connections. A typical neuron propagates electrochemical stimulation received from other neural cells using <em>dendrite</em>. An <em>axon</em> conducts electrical impulses away from the neuron (<a href="#Neuron">Neuron</a>).</p>
<div class="figure">
<img src="02-Perceptron/figures/neuron.png" title="Image Title" alt="Neuron" />
<p class="caption">Neuron</p>
</div>
<p>Expressing a computation in terms of artificial neurons was first thought in 1943, by Warren S. Mcculloch and Walter Pitts in their seminal article <em>A logical calculus of the ideas immanent in nervous activity</em>. This paper has been cited more than 14 000 times.</p>
<h2 id="perceptron-1">Perceptron</h2>
<p>A perceptron is a kind of miniature machine that produces an output for a provided input (<a href="#Perceptron">Perceptron</a>). A perceptron may accept 0, 1, or more inputs, and result in a small and simple computation. A perceptron operates on numerical values, which means that the inputs and the output are numbers (integer or float, as we will see later).</p>
<div class="figure">
<img src="02-Perceptron/figures/perceptron.png" alt="Perceptron" />
<p class="caption">Perceptron</p>
</div>
<p>The figure depicts a perceptron with three inputs, noted <em>x1</em>, <em>x2</em>, and <em>x3</em>. Each input is indicated with an incoming arrow and the output with the outgoing arrow. The $y = x^2 $ <span class="math inline">∑12<em>a</em><sup>2</sup> + <em>b</em><sup>2</sup> = <em>c</em><sup>2</sup></span></p>
<p>Not all inputs have the same importance for the perceptron. For example, an input may be more important than the others. Relevance of an input is expressed using a weight associated to that input. In our figure, the input <em>x1</em> has the weight <em>w1</em>, <em>x2</em> has the weight <em>w2</em>, and <em>x3</em> has <em>w3</em>.</p>
<p>How likely is the perceptron responding to the input stimulus? The bias is a value that indicates whether</p>
<h2 id="modeling-boolean-gates">Modeling boolean gates</h2>
<p>A perceptron is a kind of artificial neuron, developed in the 50s and 60s by Frank Rosenblatt, Warren McCulloch, and Walter Pitts.</p>
<h2 id="a-perceptron-in-action">A Perceptron in action</h2>
<pre><code>Object subclass: #Perceptron
    instanceVariableNames: &#39;weights bias&#39;
    classVariableNames: &#39;&#39;
    package: &#39;NeuralNetworks-Core&#39;</code></pre>
<pre><code>Perceptron&gt;&gt;weights: someWeightsAsNumbers
    weights := someWeightsAsNumbers copy</code></pre>
<pre><code>Perceptron&gt;&gt;weights
    ^ weights</code></pre>
<pre><code>Perceptron&gt;&gt;bias: aNumber
    bias := aNumber</code></pre>
<pre><code>Perceptron&gt;&gt;bias
    ^ bias</code></pre>
<pre><code>Perceptron&gt;&gt;feed: inputs
    | r tmp |
    tmp := inputs with: weights collect: [ :x :w | x * w ].
    r := tmp sum + bias.
    ^ r &gt; 0 ifTrue: [ 1 ] ifFalse: [ 0 ]</code></pre>
<h2 id="formulating-logical-expressions">Formulating Logical expressions</h2>
<pre><code>TestCase subclass: #NNPerceptronTest
    instanceVariableNames: &#39;&#39;
    classVariableNames: &#39;&#39;
    package: &#39;NeuralNetworksTests&#39;</code></pre>
<pre><code>NNPerceptronTest&gt;&gt;testAND
    | p |
    p := MPPerceptron new.
    p weights: { 1 . 1 }.
    p bias: -1.5.
    
    self assert: (p feed: { 0 . 0 }) equals: 0.
    self assert: (p feed: { 0 . 1 }) equals: 0.
    self assert: (p feed: { 1 . 0 }) equals: 0.
    self assert: (p feed: { 1 . 1 }) equals: 1</code></pre>
<pre><code>NNPerceptronTest&gt;&gt;testOR
    | p |
    p := MPPerceptron new.
    p weights: { 1 . 1 }.
    p bias: -0.5.
    
    self assert: (p feed: { 0 . 0 }) equals: 0.
    self assert: (p feed: { 0 . 1 }) equals: 1.
    self assert: (p feed: { 1 . 0 }) equals: 1.
    self assert: (p feed: { 1 . 1 }) equals: 1</code></pre>
<pre><code>Perceptron&gt;&gt;train: inputs desiredOutput: desiredOutput
    | c newWeights output |
    output := self feed: inputs.
    c := 0.1.
    &quot;Works well&quot;
    desiredOutput = output
        ifTrue: [ ^ self ].

    &quot;Basic check&quot;
    self assert: [ weights size = inputs size ] description: &#39;Wrong size&#39;.
    desiredOutput = 0
        ifTrue: [ &quot;we should decrease the weight&quot;
            newWeights := (1 to: weights size) collect: [ :i | (weights at: i) - (c * (inputs at: i)) ].
            bias := bias - c ]
        ifFalse: [ &quot;We have: designedOutput = 1&quot;
            newWeights := (1 to: weights size) collect: [ :i | (weights at: i) + (c * (inputs at: i)) ].
            bias := bias + c ].
    weights := newWeights</code></pre>
<pre><code>NNPerceptronTest&gt;&gt;testTrainingOR
    | p |
    p := MPPerceptron new.
    p weights: { -1 . -1 }.
    p bias: 2.
    
    100 timesRepeat: [ 
        p train: { 0 . 0 } desiredOutput: 0.
        p train: { 0 . 1 } desiredOutput: 1.
        p train: { 1 . 0 } desiredOutput: 1.
        p train: { 1 . 1 } desiredOutput: 1.
    ].
    
    self assert: (p feed: { 0 . 0 }) equals: 0.
    self assert: (p feed: { 0 . 1 }) equals: 1.
    self assert: (p feed: { 1 . 0 }) equals: 1.
    self assert: (p feed: { 1 . 1 }) equals: 1</code></pre>
<pre><code>p := MPPerceptron new.
p weights: { -1 . -1 }.
p bias: 2.
    
100 timesRepeat: [ 
    p train: { 0 . 0 } desiredOutput: 0.
    p train: { 0 . 1 } desiredOutput: 1.
    p train: { 1 . 0 } desiredOutput: 1.
    p train: { 1 . 1 } desiredOutput: 1.
].
p feed: { 1 . 0 }</code></pre>
<h2 id="exercises">Exercises</h2>
<p>The method <code>train:desiredOutput:</code> defines the learning rate <code>c</code> with the value <code>0.1</code>. Try using different values.</p>
