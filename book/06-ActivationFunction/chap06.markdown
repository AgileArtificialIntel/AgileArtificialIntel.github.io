
# Activation functions

In Chapter 3 we have seen what is the activation function. We have also see two activation functions, the step and sigmoid function. This chapter elaborates on these functions.

## Why do we need an activation function?

Why do we need an activation function? Why not simply having the output of a neuron equals to $z$? 
In that case, the output would be linear to the weighted sum of the input. And the output of a chain of neuron would actually be linear to the input. Which, at the end, is not really interesting.

It is important to have non-linear activation function

## The identity activation function


## The logistic function

The sigmoid function is a generalization of the logistic function


## The Relu activation function

